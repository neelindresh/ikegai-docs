{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Introduction","text":""},{"location":"#quick-links-to-layers","title":"Quick Links to Layers","text":"ID Layer Name Function 01 AI Data Integration, Ingestion &amp; Workflow Data processing and state orchestration. 02 AI Enrichment &amp; Foundation Model Services The cognitive core (LLM) and reasoning loops. 03 AI Data &amp; Knowledge Library/System Capabilities, APIs, and action executors. 04 Agentic AI Framework &amp; Orchestration Long-term semantic storage (RAG). 05 Ageny Tools, Skills &amp;Action Input/Output validation and safety checks. 06 Trusted AI, Risk , Governance &amp; Cyber Tracing, logging, and performance monitoring. 07 AI Observibility, Evaluation &amp; Traceability Interface between agents and the outside world. 08 AI platform DevOps &amp; FinOps Client-side interaction layer. 09 AI Application &amp; Solution CI/CD pipelines and container orchestration. 10 Interaction &amp; Experience Auditing, billing, and policy management."},{"location":"#quick-links-to-code-docs","title":"Quick Links to Code Docs","text":"Name Link Connectors Link"},{"location":"01-ai-ingestion-workflow/","title":"AI Data Integration, Ingestion &amp; Workflow","text":"<p>Module Overview</p> <p>This module serves as the central nervous system for data movement. It connects to internal and external sources to ingest data, utilizing Apache Airflow to orchestrate intelligent, event-driven, and scheduled pipelines. These pipelines reliably extract, transform, and route data for AI enrichment, predictive models, autonomous agents, and end-user applications.</p>"},{"location":"01-ai-ingestion-workflow/#sublayers","title":"Sublayers","text":"<ul> <li>1.1 : Connector Framework</li> <li>1.2 : Ingestion Pipeline &amp; Workflow Orchestration</li> <li>1.3: Change Detection &amp; Ingestion Telemetry</li> </ul>"},{"location":"01-ai-ingestion-workflow/#integration-sources","title":"Integration Sources","text":"<p>The connector layer abstracts the complexity of securely authenticating and extracting data from a diverse set of origins, categorized into three main types:</p>"},{"location":"01-ai-ingestion-workflow/#1-unstructured-data","title":"1. Unstructured Data","text":"<ul> <li>PDF Documents: Extracts text, tables, and metadata from static documents for downstream vectorization and semantic search.</li> <li>HTTP/HTTPS (Web Scraping/Crawling): Ingests raw HTML and web content from external sites and portals.</li> </ul>"},{"location":"01-ai-ingestion-workflow/#2-structured-data","title":"2. Structured Data","text":"<ul> <li>Azure SQL: Pulls relational enterprise data, seamlessly integrating with the broader Azure ecosystem.</li> <li>PostgreSQL: Connects to robust, structured transactional databases.</li> <li>SQLite: Handles lightweight, local, or microservice-specific relational data stores.</li> </ul>"},{"location":"01-ai-ingestion-workflow/#3-rest-api-based","title":"3. REST API Based","text":"<ul> <li>Subscribes to webhooks and actively polls external SaaS platforms, third-party services, and internal microservices via RESTful endpoints.</li> </ul>"},{"location":"01-ai-ingestion-workflow/#master-ingestion-architecture","title":"Master Ingestion Architecture","text":"<p>The following diagram illustrates the complete, end-to-end lifecycle of data as it passes through all three sub-layers of the integration and ingestion framework:</p> <pre><code>graph TD\n    %% Styling classes\n    classDef dbBox fill:#f0fdf4,stroke:#16a34a,stroke-width:3px,color:#0f172a,font-weight:bold;\n    classDef aiBox fill:#e0e7ff,stroke:#4f46e5,stroke-width:2px,color:#1e3a8a,font-weight:bold;\n    classDef subBox fill:#f8fafc,stroke:#0284c7,stroke-width:2px,color:#0f172a;\n\n    %% External Entities\n    User[\"User Application\"]\n    DB[(\"AI Data &amp; Knowledge Library/System\")]:::dbBox\n    AI[\"AI Data Enrichment &amp; Foundation Models\"]:::aiBox\n\n    %% The Ingestion Engine Subgraph\n    subgraph IngestionEngine [\"AI Data Integration, Ingestion &amp; Workflow\"]\n        L2[\"Sub-Layer 2: Orchestration (Airflow DAGs)\"]:::subBox\n        L3_CDC[\"Sub-Layer 3: Change Detection (CDC)\"]:::subBox\n        L1[\"Sub-Layer 1: Connector Framework\"]:::subBox\n        L3_TEL[\"Sub-Layer 3: Ingestion Telemetry\"]:::subBox\n    end\n\n    %% 1. Intake\n    User --&gt;|\"1. Uploads Raw Data\"| DB\n\n    %% 2. Internal Engine Routing\n    DB --&gt;|\"2. Event Trigger\"| L2\n    L2 --&gt;|\"3. Checks for Updates\"| L3_CDC\n    L3_CDC --&gt;|\"4. Triggers Extraction (If New)\"| L1\n\n    %% 3. AI Handoff\n    L1 ==&gt;|\"5. Sends Raw Payload\"| AI\n    AI ==&gt;|\"6. Returns Enriched Vectors/Text\"| L1\n\n    %% 4. Final Storage &amp; Observability\n    L1 --&gt;|\"7. Saves Transformed Data\"| DB\n\n    L2 -.-&gt;|\"Emits Run States\"| L3_TEL\n    L1 -.-&gt;|\"Emits Error Logs\"| L3_TEL\n    L3_TEL -.-&gt;|\"8. Centralizes All Logs\"| DB</code></pre>"},{"location":"01-ai-ingestion-workflow/#ingestion-orchestration-apache-airflow","title":"Ingestion &amp; Orchestration (Apache Airflow)","text":"<p>Data flow is strictly managed and orchestrated using Apache Airflow.</p> <ul> <li>DAG-Based Orchestration: All pipelines are defined as Directed Acyclic Graphs (DAGs) in Python, allowing for clear dependency mapping, parallel execution, and complex workflow logic.</li> <li>Batch Ingestion: Scheduled Airflow jobs handle large-scale historical data syncs, bulk updates, and routine ETL processes.</li> <li>Event-Driven Triggers: Sensors within Airflow monitor for specific events (e.g., a new PDF landing in a storage bucket or a REST webhook payload), triggering ingestion DAGs dynamically to ensure low-latency responsiveness.</li> </ul> <p>Pipeline Reliability &amp; Monitoring</p> <p>Airflow provides built-in state management. All orchestrated workflows are designed to be idempotent\u2014if a task fails, Airflow's automated retries will safely re-run the extraction or transformation without duplicating data downstream.</p>"},{"location":"02-agentic-engine/","title":"Layer 02: Agentic Engine","text":""},{"location":"02-agentic-engine/#purpose","title":"Purpose","text":"<p>The \"Brain\" of the system. This layer determines what to do based on the user intent and available context. It does not execute actions directly but plans them.</p>"},{"location":"02-agentic-engine/#key-components","title":"Key Components","text":""},{"location":"02-agentic-engine/#1-the-planner-reasoning-loop","title":"1. The Planner (Reasoning Loop)","text":"<ul> <li>ReAct / CoT: Uses Chain-of-Thought reasoning to break complex user queries into sub-tasks.</li> <li>Prompt Management: System prompts that define the agent's persona and constraints.</li> </ul>"},{"location":"02-agentic-engine/#2-context-window-management","title":"2. Context Window Management","text":"<ul> <li>Token Optimization: Summarizes conversation history to fit within the LLM's context window.</li> <li>Router: Decides which specific LLM (e.g., GPT-4 vs. Llama 3) handles the query based on complexity.</li> </ul>"},{"location":"03-ai-data-layer/","title":"Layer 03: Agentic Tool","text":""},{"location":"03-ai-data-layer/#purpose","title":"Purpose","text":"<p>The \"Hands\" of the system. This layer contains the specific functions and APIs the Agentic Engine can call to interact with the real world.</p>"},{"location":"03-ai-data-layer/#key-components","title":"Key Components","text":""},{"location":"03-ai-data-layer/#1-tool-definitions-schemas","title":"1. Tool Definitions (Schemas)","text":"<ul> <li>Function Calling: JSON schemas describing arguments (e.g., <code>get_weather(location: str)</code>).</li> <li>API Wrappers: Python/Node.js wrappers for third-party services (Salesforce, Jira, SQL).</li> </ul>"},{"location":"03-ai-data-layer/#2-execution-sandbox","title":"2. Execution Sandbox","text":"<ul> <li>Environment Safety: Tools run in isolated containers to prevent malicious code execution.</li> <li>Result Parsing: Standardizes tool outputs (JSON/Text) back into a format the Agentic Engine understands.</li> </ul>"},{"location":"01-ai-ingestion-workflow/01-connector-framework/","title":"1.1 Connector Framework","text":"<p>Framework Overview</p> <p>The Connector Framework provides a standardized, highly reusable architecture to securely integrate with any data source. Designed as a collection of modular, plug-and-play components, these modules can be shared and utilized across multiple enterprise projects without having to rewrite core extraction, processing, or authentication logic.</p>"},{"location":"01-ai-ingestion-workflow/01-connector-framework/#component-reusability-flow","title":"Component Reusability Flow","text":"<pre><code>graph TD\n    subgraph Core Reusable Components\n        K[keyvault Secrets]\n\n        C1[database]\n        C2[Document processor]\n\n        E[Vectordb &amp; Knowledge Graph]\n        L[LLM &amp; Embeddings:: Azure, Google, GCP]\n    end\n\n    subgraph Project Implementations\n        P1[IKEGAI Agentic Engine]\n        P2[Project Alpha]\n        P3[Project Beta]\n    end\n\n    CF[Connector Framework]\n\n    K -.-&gt; C1\n    K -.-&gt; C2\n    K -.-&gt; E\n    K -.-&gt; L\n\n    C1 -.-&gt; CF\n    C2 -.-&gt; CF\n    E -.-&gt; CF\n    L -.-&gt; CF\n\n\n\n\n    %% Reusability routes\n    CF ===&gt; P1\n    CF ===&gt; P2\n    CF ===&gt; P3</code></pre>"},{"location":"01-ai-ingestion-workflow/01-connector-framework/#architectural-modules","title":"Architectural Modules","text":"<p>The framework is organized into independent Python packages, ensuring strict separation of concerns between extraction, document processing, and AI instantiation.</p>"},{"location":"01-ai-ingestion-workflow/01-connector-framework/#1-authentication-security-management-keyvault","title":"1. Authentication &amp; Security Management (<code>keyvault</code>)","text":"<p>Secure connection handling is the backbone of the framework. </p> <ul> <li>Managed Identities: Utilizes Azure Key Vault (via <code>keyvault</code>) to securely fetch connection strings, API tokens, and database credentials at runtime.</li> </ul>"},{"location":"01-ai-ingestion-workflow/01-connector-framework/#2-structured-data-connectors-database","title":"2. Structured Data Connectors (<code>database/</code>)","text":"<p>Handles direct, stateful connections to relational systems (e.g., Azure SQL, PostgreSQL).</p> <ul> <li>Standardized CRUD operations and connection pooling to prevent database exhaustion during high-volume batch ingestions.</li> </ul>"},{"location":"01-ai-ingestion-workflow/01-connector-framework/#3-unstructured-document-processing-doc_processors","title":"3. Unstructured Document Processing (<code>doc_processors/</code>)","text":"<p>Dedicated to handling complex, raw files before they reach the AI orchestration layers. This module abstracts the underlying parsing engines:</p> <ul> <li> <p>Docling: Utilized for robust, open-source document parsing, layout analysis, and structuring.</p> </li> <li> <p>Azure Document Intelligence: Integrated for enterprise-grade OCR, complex table extraction, and deep document understanding.</p> </li> </ul>"},{"location":"01-ai-ingestion-workflow/01-connector-framework/#4-vector-llm-drivers-embeddings-vectordb-llm","title":"4. Vector &amp; LLM Drivers (<code>embeddings/</code>, <code>vectordb/</code>, <code>llm/</code>)","text":"<p>Once data is processed, it interfaces with agnostic driver modules designed to instantiate AI resources dynamically: - LLM Engine (<code>llm/</code>): Acts as a universal driver factory for creating LLM objects. It abstracts the provider APIs, allowing seamless switching and instantiation of models from  </p> <ul> <li> <p>Azure, Google, AWS, and Groq depending on the specific project's cost or latency requirements.</p> </li> <li> <p>Embeddings &amp; Vector DB: Transforms processed text into vector representations and manages the initialization for vector stores (e.g., Chroma DB).</p> </li> </ul>"},{"location":"01-ai-ingestion-workflow/01-connector-framework/#supported-integrations","title":"Supported Integrations","text":"<p>The framework abstracts the complexity of connecting to diverse origins, ensuring a uniform handoff to the processing layer. The supported connectors are grouped into the following domains:</p>"},{"location":"01-ai-ingestion-workflow/01-connector-framework/#1-document-file-formats","title":"1. Document &amp; File Formats","text":"<p>Designed to ingest unstructured and semi-structured static files for downstream AI OCR and text extraction.</p> <ul> <li> <p>Microsoft Word (.doc / .docx)</p> </li> <li> <p>Plain Text (.txt)</p> </li> <li> <p>PDF Files (.pdf)</p> </li> <li> <p>Microsoft Excel (.xlsx)</p> </li> </ul>"},{"location":"01-ai-ingestion-workflow/01-connector-framework/#2-relational-databases","title":"2. Relational Databases","text":"<p>Handles direct, stateful connections to enterprise transactional and analytical databases, supporting both full loads and CDC (Change Data Capture).</p> <ul> <li> <p>Azure SQL</p> </li> <li> <p>PostgreSQL</p> </li> <li> <p>MySQL</p> </li> <li> <p>Oracle Database</p> </li> </ul>"},{"location":"01-ai-ingestion-workflow/01-connector-framework/#3-cloud-storage-data-platforms","title":"3. Cloud Storage &amp; Data Platforms","text":"<p>Integrates seamlessly with major cloud providers and big data platforms for high-volume batch ingestion.</p> <ul> <li> <p>Amazon S3</p> </li> <li> <p>Azure Data Lake</p> </li> <li> <p>Google Cloud</p> </li> <li> <p>Databricks</p> </li> </ul>"},{"location":"01-ai-ingestion-workflow/01-connector-framework/#4-vector-stores-knowledge-bases","title":"4. Vector Stores &amp; Knowledge Bases","text":"<p>Connects to specialized storage systems used for retrieval-augmented generation (RAG) and semantic search.</p> <ul> <li> <p>Pinecone</p> </li> <li> <p>Internal Knowledge Bases</p> </li> </ul>"},{"location":"01-ai-ingestion-workflow/01-connector-framework/#5-web-apis","title":"5. Web &amp; APIs","text":"<ul> <li>HTTP/HTTPS: Actively polls third-party SaaS platforms, RESTful APIs, and scrapes raw web content.</li> </ul>"},{"location":"01-ai-ingestion-workflow/01-connector-framework/#python-sdk-examples","title":"Python SDK examples","text":"<ol> <li>Invoke Connectors</li> </ol> <pre><code>from connectors.database.mongo_retrive import MongoConnect\n\ngraph_db=MongoConnect(MongoConfig.uri,UsecaseInfo.db,\"agent_graph_store\")\n\n\ngraph_db.get_client()\ngraph_db.get_collection()\ngraph_db.fetch_one(idx=\"idx123\")\ngraph_db.fetch_all()\n...\n</code></pre> <p>Note : For a comprehensive documentation visit Connector docs </p>"},{"location":"01-ai-ingestion-workflow/02-data-ingestion-and-workflow/","title":"Ingestion Pipeline &amp; Workflow Orchestration","text":"<p>Module Overview</p> <p>This layer acts as the operational brain of the ingestion framework. It coordinates data movement end-to-end across both batch and real-time modes. By combining event-driven triggering, precise buffering control, and highly reliable execution semantics (such as automated retries, checkpointing, and historical backfills), it ensures that downstream AI agents and models receive a continuous, unbroken stream of clean data.</p>"},{"location":"01-ai-ingestion-workflow/02-data-ingestion-and-workflow/#core-orchestration-engine","title":"Core Orchestration Engine","text":"<p>All workflow coordination is managed centrally by Apache Airflow. Instead of isolated cron jobs or manual scripts, every ingestion pipeline is defined as a Directed Acyclic Graph (DAG) using Python. This approach ensures that tasks connect logically while maintaining strict independence and fault tolerance. </p>"},{"location":"01-ai-ingestion-workflow/02-data-ingestion-and-workflow/#execution-semantics-reliability","title":"Execution Semantics &amp; Reliability","text":"<p>To ensure enterprise-grade reliability, the orchestration layer implements several critical execution semantics:</p>"},{"location":"01-ai-ingestion-workflow/02-data-ingestion-and-workflow/#1-job-control-triggering","title":"1. Job Control &amp; Triggering","text":"<p>Pipelines operate dynamically based on the ingestion mode:</p> <ul> <li> <p>Batch Mode: Workflows execute on a predefined time schedule (e.g., daily warehouse refreshes).</p> </li> <li> <p>Real-Time / Event-Driven: Workflows are triggered instantly via Airflow Sensors or external API calls when new data arrives.</p> </li> </ul>"},{"location":"01-ai-ingestion-workflow/02-data-ingestion-and-workflow/#2-automated-retries","title":"2. Automated Retries","text":"<p>Data pipelines are prone to transient errors such as network blips or temporary API rate limits.</p> <ul> <li> <p>Airflow is configured to automatically retry failed tasks, executing them from the beginning. </p> </li> <li> <p>The engine utilizes tunable parameters like <code>retries</code> and <code>retry_delay</code>, often employing exponential backoff to prevent overwhelming downstream systems during an outage.</p> </li> </ul>"},{"location":"01-ai-ingestion-workflow/02-data-ingestion-and-workflow/#3-state-management-checkpointing","title":"3. State Management &amp; Checkpointing","text":"<p>Because Airflow tasks are stateless, independent units, state tracking is handled externally. </p> <ul> <li> <p>Execution status is constantly checkpointed and recorded in <code>AI Data &amp; Knowledge Library/System</code></p> </li> <li> <p>If a worker node crashes mid-execution, the system knows exactly which tasks succeeded and which need to be restarted.</p> </li> </ul>"},{"location":"01-ai-ingestion-workflow/02-data-ingestion-and-workflow/#buffering-concurrency-control","title":"Buffering &amp; Concurrency Control","text":"<p>When processing massive historical backfills or handling sudden spikes in real-time events, the orchestrator prevents infrastructure strain through strict concurrency limits. </p> <ul> <li> <p>Active Run Limits: The <code>max_active_runs</code> parameter restricts how many DAG runs can execute simultaneously.</p> </li> <li> <p>Task Pools: Specific resource-intensive connections (like the Azure SQL database connector) are assigned to isolated Airflow pools, ensuring they never exceed connection limits or cause database exhaustion.</p> </li> </ul>"},{"location":"01-ai-ingestion-workflow/02-data-ingestion-and-workflow/#orchestration-flow-ai-enrichment-diagram","title":"Orchestration Flow &amp; AI Enrichment Diagram","text":"<p>While Apache Airflow handles the scheduling and coordination of the pipelines, it does not do the heavy lifting itself. Instead, the DAG nodes actively utilize the Connector Framework (detailed in Section 1.1) to execute the extractions. </p> <p>When complex processing is required, the connectors securely hand off the payloads to the AI Data Enrichment &amp; Foundation Model Services layer. This external layer performs the high-compute tasks\u2014such as OCR, semantic chunking, and LLM inference\u2014before returning the normalized data to the pipeline for final storage.</p> <pre><code>graph TD\n    %% Custom Styles for visual weight\n    classDef boldBox fill:#e2e8f0,stroke:#475569,stroke-width:2px,color:#0f172a,font-weight:bold;\n    classDef aiBox fill:#e0e7ff,stroke:#4f46e5,stroke-width:2px,color:#1e3a8a,font-weight:bold;\n    classDef toolBox fill:#f8fafc,stroke:#38bdf8,stroke-width:2px,color:#0f172a,font-weight:bold;\n\n    subgraph AppLayer [\"User Application Layer\"]\n        U((&lt;b&gt;User&lt;/b&gt;)) --&gt;|Interacts| A[&lt;b&gt;Application Build&lt;/b&gt;]\n        A --&gt;|Selects Context| D[&lt;b&gt;Dataset&lt;/b&gt;]\n        D --&gt;|Uploads/Submits| Input[&lt;b&gt;Document File / Target URL&lt;/b&gt;]\n    end\n\n    subgraph Core Storage\n        S1[(&lt;b&gt;AI Data &amp; Knowledge Library&lt;/b&gt;)]:::boldBox\n    end\n\n    subgraph Airflow Orchestration\n        DAG1{&lt;b&gt;doc_ingestion DAG&lt;/b&gt;}:::boldBox\n        DAG2{&lt;b&gt;url_ingestion DAG&lt;/b&gt;}:::boldBox\n    end\n\n    subgraph ExecutionLayer [\"Execution Layer\"]\n        CF[&lt;b&gt;Connector Framework&lt;/b&gt;]:::boldBox\n    end\n\n    subgraph EnrichmentLayer [\"Enrichment\"]\n        AIE[&lt;b&gt;AI Data Enrichment &amp; &lt;br/&gt;Foundation Model Services&lt;/b&gt;]:::aiBox\n    end\n\n    subgraph ToolsLayer [\"Tools\"]\n        direction LR\n        LLM[&lt;b&gt;LLM &amp; Embeddings&lt;/b&gt;]:::toolBox\n        VDB[&lt;b&gt;Vector Database&lt;/b&gt;]:::toolBox\n        DI[&lt;b&gt;Azure Document Intelligence&lt;/b&gt;]:::toolBox\n    end\n\n    %% Initial Storage &amp; Trigger\n    Input --&gt; S1\n    S1 --&gt;|Event Trigger| DAG1\n    S1 --&gt;|Event Trigger| DAG2\n\n    %% Connectors Execution\n    DAG1 --&gt;|Invokes Tasks| CF\n    DAG2 --&gt;|Invokes Tasks| CF\n\n    %% Layer Connections matching the architectural mockup\n    CF -.-&gt;|Connects To| AIE\n\n    AIE --&gt;|Calls to| LLM\n    AIE --&gt;|Calls to| VDB\n    AIE --&gt;|Calls to| DI\n\n    %% Final Storage &amp; State Management\n    CF --&gt;|Saves Transformed Data| S1\n    DAG1 -.-&gt;|Saves Node States &amp; Logs| S1\n    DAG2 -.-&gt;|Saves Node States &amp; Logs| S1\n\n</code></pre>"},{"location":"01-ai-ingestion-workflow/02-data-ingestion-and-workflow/#active-dag-pipelines","title":"Active DAG Pipelines","text":"<p>The orchestration engine currently relies on two primary data ingestion pipelines, each tailored to specific data formats.</p>"},{"location":"01-ai-ingestion-workflow/02-data-ingestion-and-workflow/#1-document-ingestion-pipeline-doc_ingestion","title":"1. Document Ingestion Pipeline (<code>doc_ingestion</code>)","text":"<p>This complex DAG is responsible for securely pulling, parsing, and transforming uploaded files. It utilizes branching logic to intelligently route data based on extraction success.</p> <ul> <li><code>get_data_from_queue</code>: Picks up the new document processing event triggered by the user upload.</li> <li><code>process_document</code>: Hands the raw file off to the <code>doc_processors</code> (like Docling or Azure Document Intelligence) to extract text and markdown.</li> <li><code>check_metadata_extraction</code> (Branch): A conditional logic node. It verifies if the processor successfully extracted the required metadata and layout structure.</li> <li><code>extract_metadata</code> &amp; <code>transform</code>: Cleanses the text, chunks the markdown, and prepares the payload for vectorization.</li> <li><code>save_to_mongo</code>: Commits the final transformed data and structured embeddings to the database layer.</li> </ul>"},{"location":"01-ai-ingestion-workflow/02-data-ingestion-and-workflow/#2-url-ingestion-pipeline-url_ingestion","title":"2. URL Ingestion Pipeline (<code>url_ingestion</code>)","text":"<p>A streamlined, linear DAG designed for fast web scraping and data syncing.</p> <ul> <li><code>get_url</code>: Receives the target URL submitted from the dataset application.</li> <li><code>load_data</code>: Uses HTTP connectors to scrape the raw HTML or fetch the target payload.</li> <li><code>update_mongo</code>: Directly updates the existing records in the database with the newly fetched web content.</li> </ul>"},{"location":"01-ai-ingestion-workflow/02-data-ingestion-and-workflow/#state-management-observability","title":"State Management &amp; Observability","text":"<p>To ensure enterprise-grade reliability, Airflow's internal tracking is deeply integrated with your custom infrastructure. </p> <p>Instead of relying solely on Airflow's default backend for monitoring, all logs and states are saved directly into the <code>AI Data &amp; Knowledge Library/System</code>. Because the status of every single task node is centralized here, the frontend application can seamlessly query the library to show users real-time progress bars or error alerts if an ingestion step (like <code>extract_metadata</code>) fails.</p>"},{"location":"01-ai-ingestion-workflow/02-data-ingestion-and-workflow/#sample-screen-shots-of-airflow","title":"Sample Screen Shots of Airflow","text":"<p>Documentation Airflow Documentation</p>"},{"location":"01-ai-ingestion-workflow/03-change-detection-telemetry/","title":"Sub-Layer 3: Change Detection &amp; Ingestion Telemetry","text":"<p>Module Overview</p> <p>This layer guarantees data freshness and system observability. It intelligently tracks source-level modifications to prevent duplicate processing, while emitting rich telemetry to provide complete operational visibility\u2014spanning run health, error diagnostics, and end-to-end data lineage.</p>"},{"location":"01-ai-ingestion-workflow/03-change-detection-telemetry/#telemetry-change-detection-flow","title":"Telemetry &amp; Change Detection Flow","text":"<pre><code>graph TD\n    classDef sourceBox fill:#f8fafc,stroke:#94a3b8,stroke-width:2px;\n    classDef logicBox fill:#fef08a,stroke:#ca8a04,stroke-width:2px,color:#854d0e,font-weight:bold;\n    classDef obsBox fill:#e0e7ff,stroke:#4f46e5,stroke-width:2px,color:#1e3a8a,font-weight:bold;\n    classDef dbBox fill:#f0fdf4,stroke:#16a34a,stroke-width:3px,color:#0f172a,font-weight:bold;\n\n    subgraph Sources [\"Data Sources\"]\n        DB[(Relational DBs)]:::sourceBox\n        DOC[Web &amp; Files]:::sourceBox\n    end\n\n    subgraph CDC [\"Change Detection Layer\"]\n        L[Log Reader / WAL]:::logicBox\n        T[Timestamp Polling]:::logicBox\n    end\n\n    subgraph Execution [\"Airflow Pipeline\"]\n        PROC[Data Transformation &amp; Chunking]\n    end\n\n    subgraph Storage [\"AI Data &amp; Knowledge Library/System\"]\n        M[Metrics: Throughput &amp; Latency]:::obsBox\n        LG[Logs: Task State &amp; Errors]:::obsBox\n        TR[Traces: Data Lineage Graph]:::obsBox\n        VEC[(Versioned Artifacts &amp; Vectors)]:::dbBox\n    end\n\n    DB --&gt; L\n    DOC --&gt; T\n\n    L --&gt;|New/Modified Rows| PROC\n    T --&gt;|Recent Payloads| PROC\n\n    %% Telemetry &amp; Data Emission to the Library/System\n    PROC --&gt;|Saves Active/Inactive Data| VEC\n    PROC -.-&gt;|Emits Execution Time| M\n    PROC -.-&gt;|Emits Node Status| LG\n    PROC -.-&gt;|Emits Source-to-Vector Path| TR</code></pre>"},{"location":"01-ai-ingestion-workflow/03-change-detection-telemetry/#change-detection-version-control-cdc","title":"Change Detection &amp; Version Control (CDC)","text":"<p>Instead of repeatedly extracting entire tables or re-parsing unchanged documents, the ingestion pipeline utilizes Change Data Capture (CDC) methodologies. CDC identifies and records row-level changes\u2014such as inserts, updates, and deletes\u2014and applies only those modified records to downstream systems. This minimizes compute overhead and ensures downstream AI agents always have the freshest context.</p>"},{"location":"01-ai-ingestion-workflow/03-change-detection-telemetry/#detection-strategies","title":"Detection Strategies","text":"<p>Depending on the data source, the framework applies targeted detection mechanisms to prevent duplicate ingestion:</p> <ul> <li>Log-Based CDC (Databases): For relational databases mapped in your <code>database/</code> connectors (like Azure SQL or PostgreSQL), the framework reads the native database transaction logs to capture changes. This is the most efficient approach as it captures all changes, including deletes, with zero impact on application queries.</li> <li>Timestamp Polling (APIs, Web &amp; Documents): For REST APIs, web scraping, and file storage, the pipeline queries the <code>updated_at</code> timestamps or last-modified metadata to fetch only the newly added or modified payloads.</li> </ul>"},{"location":"01-ai-ingestion-workflow/03-change-detection-telemetry/#versioning-deletion-handling","title":"Versioning &amp; Deletion Handling","text":"<p>When a record or document is deleted at the source, it is captured as a specific change event. To maintain a consistent version history and prevent broken references in the AI's Retrieval-Augmented Generation (RAG) memory, the system employs soft deletes (tombstoning). The vector embeddings and metadata stored within the <code>AI Data &amp; Knowledge Library/System</code> are marked as inactive rather than hard-deleted, preserving the historical audit trail.</p>"},{"location":"01-ai-ingestion-workflow/03-change-detection-telemetry/#ingestion-telemetry-observability","title":"Ingestion Telemetry &amp; Observability","text":"<p>Telemetry is the automated, continuous measurement and transmission of data from systems to a central place for analysis. Because pipelines can fail silently or experience upstream schema drift, this layer emits continuous signals to ensure the data is complete, accurate, and trustworthy.</p> <p>The pipeline observability relies on three core types of telemetry data, all of which are centralized in your storage layer:</p>"},{"location":"01-ai-ingestion-workflow/03-change-detection-telemetry/#1-metrics-health-performance","title":"1. Metrics (Health &amp; Performance)","text":"<p>Numerical measurements collected over time that reflect the state of the ingestion system. </p> <ul> <li> <p>Run Tracking: Tracks DAG execution times, task queue lengths, and data volume throughput.</p> </li> <li> <p>Error Rates: Monitors the frequency of API timeouts or database connection drops to proactively detect system degradation.</p> </li> </ul>"},{"location":"01-ai-ingestion-workflow/03-change-detection-telemetry/#2-logs-error-diagnostics","title":"2. Logs (Error Diagnostics)","text":"<p>Timestamped records of events and errors produced by the software.</p> <ul> <li> <p>Logs provide the rich context essential for root cause analysis. </p> </li> <li> <p>As established in Sub-Layer 2, the status, logs, and execution states of every single Airflow task node are pushed directly into the <code>AI Data &amp; Knowledge Library/System</code>, ensuring centralized, easily searchable error diagnostics.</p> </li> </ul>"},{"location":"01-ai-ingestion-workflow/03-change-detection-telemetry/#3-traces-lineage-auditability","title":"3. Traces &amp; Lineage (Auditability)","text":"<p>A trace represents the journey of data as it moves through the distributed system. Data lineage provides end-to-end transparency.</p> <ul> <li> <p>Impact Analysis: Lineage reveals exactly which downstream datasets or AI models are affected by an upstream issue.</p> </li> <li> <p>Debugging Context: If a LangChain agent hallucinates or retrieves questionable data, engineers can trace that specific vector back through the pipeline to find the exact source document, extraction timestamp, and transformation node. </p> </li> <li> <p>Compliance: This strict tracking ensures the system can demonstrate exactly where sensitive data originated and how it was processed, which is a requirement for enterprise governance.</p> </li> </ul>"},{"location":"appendix/code_docs/code_docs/","title":"Connector Code Reference","text":"<p>Overview</p> <p>This section contains the detailed, code-level documentation for the individual modules within the IKEGAI Connector Framework. Here you will find class definitions, method signatures, and implementation guides for interacting with our supported databases, document parsers, and external APIs.</p>"},{"location":"appendix/code_docs/code_docs/#available-modules","title":"Available Modules","text":"<p>Select a module below to view its specific developer documentation and usage examples.</p>"},{"location":"appendix/code_docs/code_docs/#database-connectors","title":"\ud83d\uddc4\ufe0f Database Connectors","text":"<p>Modules responsible for stateful, structured data interactions, connection pooling, and standard CRUD operations.</p> <ul> <li>MongoDB Connector (<code>MongoConnect</code>) Details the multiton-pattern MongoDB wrapper used for resilient, efficient database connections and automatic ObjectId handling.</li> </ul>"},{"location":"appendix/code_docs/code_docs/#document-processors","title":"\ud83d\udcc4 Document Processors","text":"<p>Modules responsible for unstructured file parsing, OCR, and layout analysis before passing data to the LLM and embedding layers.</p> <ul> <li>Azure Document Intelligence (<code>AzureDocProcessor</code>) Details the Azure AI wrapper used to extract clean Markdown, tables, and page-wise text chunks from complex documents like PDFs.</li> </ul>"},{"location":"appendix/code_docs/code_docs/#embeddings-rerankers","title":"\ud83e\udde0 Embeddings &amp; Rerankers","text":"<p>Modules responsible for transforming text into vector representations using optimized caching and factory patterns.</p> <ul> <li> <p>Azure OpenAI Embeddings (<code>AzureOpenAIEmbeddingsLoader</code>) Details the Singleton-style factory class used to load and cache LangChain Azure OpenAI embedding models to optimize application memory and performance.</p> </li> <li> <p>HuggingFace Embeddings (<code>HuggingFaceEmbeddingsLoader</code>) Details the local caching factory for standard HuggingFace bi-encoder embedding models.</p> </li> <li> <p>HuggingFace Reranker (<code>HuggingFaceRerankerLoader</code>) Details the local caching factory for heavy HuggingFace cross-encoder models used in context compression and document reranking.</p> </li> </ul> <p>Expanding the Framework</p> <p>This directory will be continuously updated as new code references are generated for our wider ecosystem of supported data sources (e.g., Azure SQL, Postgres, Amazon S3, and standard HTTP web scrapers).</p>"},{"location":"appendix/code_docs/connectors/azuredoc/","title":"Azure Document Intelligence Processor (<code>AzureDocProcessor</code>)","text":"<p>Class Overview</p> <p><code>AzureDocProcessor</code> is a dedicated wrapper for the Azure Document Intelligence API. It utilizes the <code>prebuilt-layout</code> model to analyze complex documents (PDFs, images) and extract text, tables, and structural elements directly into Markdown format. This makes it an ideal precursor for LLM ingestion and Vector DB chunking.</p>"},{"location":"appendix/code_docs/connectors/azuredoc/#initialization","title":"Initialization","text":"<p>The class requires an active Azure endpoint and API key to instantiate the <code>DocumentIntelligenceClient</code>.</p> <p><code>__init__(self, endpoint: str, key: str)</code></p> <ul> <li><code>endpoint</code>: The URL of your Azure Document Intelligence resource.</li> <li><code>key</code>: The corresponding API access key.</li> </ul>"},{"location":"appendix/code_docs/connectors/azuredoc/#class-methods","title":"Class Methods","text":""},{"location":"appendix/code_docs/connectors/azuredoc/#1-full-document-extraction","title":"1. Full Document Extraction","text":"<p><code>analyze_layout_with_markdown(self, file_path: str) -&gt; str</code></p> <p>Reads a local document and returns the entire parsed content as a single, continuous Markdown string.</p> <ul> <li>Arguments:<ul> <li><code>file_path</code> (str): The local path to the file (e.g., PDF, PNG, JPEG) to be analyzed.</li> </ul> </li> <li>Returns: * <code>str</code>: The complete Markdown representation of the document.</li> <li>Use Case: Best suited for smaller documents or when you want the LangChain framework to handle all of the recursive text chunking dynamically.</li> </ul>"},{"location":"appendix/code_docs/connectors/azuredoc/#2-page-wise-extraction","title":"2. Page-Wise Extraction","text":"<p><code>analyze_layout_with_markdown_pagewise(self, file_path: str) -&gt; list[dict]</code></p> <p>Extracts the document and intelligently splits the Markdown output strictly by the document's original page boundaries. </p> <p>How Page Slicing Works</p> <p>The Azure API returns a single giant string for the whole document alongside an array of page objects. This method iterates through <code>result.pages</code> and uses the <code>span</code> attributes (offset and length) to precisely slice the global Markdown string into page-specific chunks.</p> <ul> <li>Arguments:<ul> <li><code>file_path</code> (str): The local path to the file.</li> </ul> </li> <li>Returns: * A list of dictionaries, where each dictionary represents a single page.<ul> <li>Format: <code>[{\"page_no\": 1, \"content\": \"## Header\\n\\nPage 1 text...\"}]</code></li> </ul> </li> <li>Use Case: Essential for Retrieval-Augmented Generation (RAG) pipelines where maintaining exact page-level citations and metadata is required for the LLM's response.</li> </ul>"},{"location":"appendix/code_docs/connectors/azuredoc/#usage-example","title":"Usage Example","text":"<pre><code>from doc_processors import AzureDocProcessor\n\n# Initialize the processor\nprocessor = AzureDocProcessor(\n    endpoint=\"[https://your-resource.cognitiveservices.azure.com/](https://your-resource.cognitiveservices.azure.com/)\",\n    key=\"your_azure_api_key\"\n)\n\nfile_to_process = \"assets/documents/financial_report_2025.pdf\"\n\n# Example 1: Get the full document as one string\nfull_markdown = processor.analyze_layout_with_markdown(file_to_process)\nprint(full_markdown[:500]) # Print the first 500 characters\n\n# Example 2: Get page-by-page chunks\npaged_markdown = processor.analyze_layout_with_markdown_pagewise(file_to_process)\n\nfor page in paged_markdown:\n    print(f\"--- Document Page: {page['page_no']} ---\")\n    print(page['content'])\n</code></pre>"},{"location":"appendix/code_docs/connectors/embedding_azureopenai/","title":"Azure OpenAI Embeddings (<code>AzureOpenAIEmbeddingsLoader</code>)","text":"<p>Class Overview</p> <p><code>AzureOpenAIEmbeddingsLoader</code> is a specialized factory class that inherits from <code>EmbeddingsFunction</code>. It provides a highly optimized wrapper around LangChain's <code>AzureOpenAIEmbeddings</code>. By utilizing a class-level caching mechanism, it ensures that identical embedding models are only loaded into memory once across the entire application lifespan.</p>"},{"location":"appendix/code_docs/connectors/embedding_azureopenai/#core-architecture-the-factory-caching-pattern","title":"Core Architecture: The Factory &amp; Caching Pattern","text":"<p>The most critical feature of this class is its use of the Python <code>__new__</code> dunder method. Unlike standard classes that return an instance of themselves, this loader intercepts the creation process to act as a Singleton-style Factory.</p> <ol> <li>Cache Check: When called, <code>__new__</code> checks an internal class-level dictionary (<code>_store</code>) to see if an instance of the requested <code>model_name</code> already exists.</li> <li>First-Time Load: If the model is not in the cache, it initializes a brand new LangChain <code>AzureOpenAIEmbeddings</code> object, saves it to <code>_store</code>, and returns it.</li> <li>Subsequent Loads: If the model is already in the cache, it bypasses initialization entirely and returns the existing LangChain model object.</li> </ol> <p>Python Object Returns</p> <p>Because <code>__new__</code> explicitly returns an instance of <code>AzureOpenAIEmbeddings</code> (and not <code>AzureOpenAIEmbeddingsLoader</code>), the <code>__init__</code> method of this loader class is effectively bypassed. You are receiving the raw, configured LangChain object ready for immediate use in vector stores or RAG pipelines.</p>"},{"location":"appendix/code_docs/connectors/embedding_azureopenai/#initialization-parameters","title":"Initialization Parameters","text":"<p>When calling the loader, you must provide the necessary Azure connection details. </p> Parameter Type Description <code>model_name</code> <code>str</code> The specific name of the embedding model (e.g., <code>text-embedding-ada-002</code>). This acts as the unique cache key. <code>api_key</code> <code>str</code> Your secure Azure OpenAI API key. <code>base_url</code> <code>str</code> The base endpoint URL for your Azure OpenAI resource. <code>api_version</code> <code>str</code> The API version to target (e.g., <code>2023-05-15</code>). <code>deployment_name</code> <code>str</code> The custom deployment name you assigned to the model in Azure AI Studio."},{"location":"appendix/code_docs/connectors/embedding_azureopenai/#caching-flow-diagram","title":"Caching Flow Diagram","text":"<pre><code>sequenceDiagram\n    participant App as Application\n    participant Loader as AzureOpenAIEmbeddingsLoader\n    participant Cache as cls._store\n    participant Azure as LangChain AzureOpenAIEmbeddings\n\n    App-&gt;&gt;Loader: Call Loader(model_name=\"ada-002\", ...)\n    Loader-&gt;&gt;Cache: Check if \"ada-002\" exists\n\n    alt Model Not in Cache\n        Cache--&gt;&gt;Loader: False\n        Loader-&gt;&gt;Azure: Instantiate new model\n        Azure--&gt;&gt;Loader: Return Model Instance\n        Loader-&gt;&gt;Cache: Save Instance to _store[\"ada-002\"]\n        Loader--&gt;&gt;App: Return Model Instance\n    else Model In Cache\n        Cache--&gt;&gt;Loader: True (Returns existing Instance)\n        Loader--&gt;&gt;App: Return Cached Instance (Bypasses __init__)\n    end</code></pre>"},{"location":"appendix/code_docs/connectors/embedding_reranker_hugginface/","title":"HuggingFace Reranker (<code>HuggingFaceRerankerLoader</code>)","text":"<p>Class Overview</p> <p><code>HuggingFaceRerankerLoader</code> is a factory class designed to optimize the initialization of LangChain's <code>HuggingFaceCrossEncoder</code>. By implementing a Singleton-style caching mechanism, it ensures that heavy cross-encoder models\u2014which evaluate the relevance between pairs of texts\u2014are only loaded into memory once, significantly reducing latency and memory overhead across your application.</p>"},{"location":"appendix/code_docs/connectors/embedding_reranker_hugginface/#core-architecture-cross-encoder-caching","title":"Core Architecture: Cross-Encoder Caching","text":"<p>While standard embedding models (bi-encoders) map text to vectors independently, a cross-encoder processes the query and the document simultaneously through a shared neural network to output a highly accurate similarity score. Because these models are computationally expensive, loading them repeatedly is a massive bottleneck.</p> <p>Similar to our embedding loaders, this class intercepts object creation using the <code>__new__</code> dunder method:</p> <ul> <li>Cache Check: When instantiated, <code>__new__</code> checks the internal <code>_store</code> to see if the requested cross-encoder model already exists in memory.</li> <li>First-Time Load: If absent, it initializes a <code>HuggingFaceCrossEncoder</code> object (downloading/loading the weights) and caches it.</li> <li>Subsequent Loads: If present, it bypasses initialization and instantly returns the cached LangChain model object.</li> </ul>"},{"location":"appendix/code_docs/connectors/embedding_reranker_hugginface/#initialization-parameters","title":"Initialization Parameters","text":"Parameter Type Description <code>model_name</code> <code>str</code> The HuggingFace repository ID for the cross-encoder (e.g., <code>BAAI/bge-reranker-large</code> or <code>cross-encoder/ms-marco-MiniLM-L-6-v2</code>). <code>model_kwargs</code> <code>dict</code> (Optional) Hardware configuration arguments (e.g., <code>{\"device\": \"cuda\"}</code>)."},{"location":"appendix/code_docs/connectors/embedding_reranker_hugginface/#usage-example-with-langchain","title":"Usage Example (with LangChain)","text":"<p>Because this factory returns the raw LangChain <code>HuggingFaceCrossEncoder</code> object, you can plug it directly into LangChain's <code>CrossEncoderReranker</code> to compress and reorder your retrieved documents.</p> <pre><code>from connectors.embeddings.huggingface_reranker import HuggingFaceRerankerLoader\nfrom langchain.retrievers.document_compressors import CrossEncoderReranker\nfrom langchain.retrievers import ContextualCompressionRetriever\n\n# 1. Load the cross-encoder model (caches on first run)\nhf_cross_encoder = HuggingFaceRerankerLoader(\n    model_name=\"BAAI/bge-reranker-large\",\n    model_kwargs={\"device\": \"cpu\"}\n)\n\n# 2. Wrap it in a LangChain Document Compressor\ncompressor = CrossEncoderReranker(model=hf_cross_encoder, top_n=3)\n\n# 3. Attach it to your existing base retriever (e.g., Qdrant or Chroma)\ncompression_retriever = ContextualCompressionRetriever(\n    base_compressor=compressor,\n    base_retriever=your_base_vector_retriever\n)\n\n# 4. Invoke the pipeline (documents are fetched, then intelligently reranked)\nfinal_docs = compression_retriever.invoke(\"What is the architecture of the agentic engine?\")\n</code></pre>"},{"location":"appendix/code_docs/connectors/embeddings_hugginface/","title":"HuggingFace Embeddings (<code>HuggingFaceEmbeddingsLoader</code>)","text":"<p>Class Overview</p> <p><code>HuggingFaceEmbeddingsLoader</code> is a specialized factory class that inherits from <code>EmbeddingsFunction</code>. It provides a highly optimized wrapper around LangChain's <code>HuggingFaceEmbeddings</code>. By utilizing a class-level caching mechanism, it ensures that heavy open-source embedding models (like <code>sentence-transformers</code>) are loaded into the application's RAM/VRAM only once.</p>"},{"location":"appendix/code_docs/connectors/embeddings_hugginface/#core-architecture-local-model-caching","title":"Core Architecture: Local Model Caching","text":"<p>Loading local HuggingFace models can be slow and memory-intensive. Similar to our Azure implementation, this class intercepts the creation process using the Python <code>__new__</code> dunder method to act as a Singleton-style Factory.</p> <ol> <li>Cache Check: When called, <code>__new__</code> checks an internal class-level dictionary (<code>_store</code>) to see if an instance of the requested <code>model_name</code> already exists in memory.</li> <li>First-Time Load: If the model is not in the cache, it initializes a brand new LangChain <code>HuggingFaceEmbeddings</code> object (which loads the weights into memory), saves it to <code>_store</code>, and returns it.</li> <li>Subsequent Loads: If the model is already in the cache, it bypasses initialization entirely and instantly returns the existing model object.</li> </ol> <p>Python Object Returns</p> <p>Just like the Azure loader, <code>__new__</code> explicitly returns an instance of the LangChain <code>HuggingFaceEmbeddings</code> object, effectively bypassing this loader's <code>__init__</code> method.</p>"},{"location":"appendix/code_docs/connectors/embeddings_hugginface/#initialization-parameters","title":"Initialization Parameters","text":"Parameter Type Description <code>model_name</code> <code>str</code> The specific HuggingFace model repo ID (e.g., <code>sentence-transformers/all-MiniLM-L6-v2</code>). This acts as the unique cache key. <code>model_kwargs</code> <code>dict</code> (Optional) Dictionary of keyword arguments to pass to the model (e.g., <code>{\"device\": \"cuda\"}</code>). <code>encode_kwargs</code> <code>dict</code> (Optional) Dictionary of keyword arguments to pass when encoding text (e.g., <code>{\"normalize_embeddings\": True}</code>)."},{"location":"appendix/code_docs/connectors/embeddings_hugginface/#usage-example","title":"Usage Example","text":"<pre><code>from connectors.embeddings.huggingface import HuggingFaceEmbeddingsLoader\n\n# 1. First invocation: Downloads (if necessary) and loads the heavy model into memory\nhf_embeddings = HuggingFaceEmbeddingsLoader(\n    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n    model_kwargs={\"device\": \"cpu\"},\n    encode_kwargs={\"normalize_embeddings\": False}\n)\n# Console Output: Loading sentence-transformers/all-MiniLM-L6-v2 for the first time...\n\n# 2. Second invocation elsewhere: Instantly fetches from RAM without reloading weights\nsame_hf_embeddings = HuggingFaceEmbeddingsLoader(\n    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n)\n# Console Output: Fetching sentence-transformers/all-MiniLM-L6-v2 from cache...\n\n# 3. Use standard LangChain methods\nvectors = hf_embeddings.embed_query(\"How does caching improve performance?\")\n</code></pre>"},{"location":"appendix/code_docs/connectors/mongo/","title":"MongoDB Connector (<code>MongoConnect</code>)","text":"<p>Class Overview</p> <p><code>MongoConnect</code> is a robust, resilient Python wrapper for <code>pymongo.MongoClient</code>. It implements a Multiton Design Pattern to manage database connections efficiently, ensuring that only a single active connection exists for any unique combination of <code>(URI, Database, Collection)</code>. </p>"},{"location":"appendix/code_docs/connectors/mongo/#core-architecture-the-multiton-pattern","title":"Core Architecture: The Multiton Pattern","text":"<p>Instead of creating a new database connection every time the class is instantiated, <code>MongoConnect</code> intercepts the creation process using the <code>__new__</code> dunder method. </p> <ol> <li>State Check: It checks an internal <code>_instances</code> dictionary for an existing connection matching the requested URI, DB, and Collection.</li> <li>Health Check: If an instance exists, it actively pings the MongoDB server (<code>if_alive()</code>) to ensure the connection hasn't dropped or timed out.</li> <li>Smart Instantiation: It only creates a new <code>MongoClient</code> connection if no instance exists, or if the existing connection is dead.</li> </ol>"},{"location":"appendix/code_docs/connectors/mongo/#class-methods","title":"Class Methods","text":""},{"location":"appendix/code_docs/connectors/mongo/#connection-management","title":"Connection Management","text":"Method Description <code>get_client()</code> Returns the raw <code>pymongo.MongoClient</code> connection object. <code>get_collection()</code> Returns the specific <code>pymongo.collection.Collection</code> object instantiated for this class. <code>if_alive()</code> Pings the MongoDB <code>admin</code> database. Returns <code>True</code> if the connection is healthy, and <code>False</code> if it has dropped. <code>_connect()</code> An internal helper method to forcefully re-establish the connection to the URI, Database, and Collection."},{"location":"appendix/code_docs/connectors/mongo/#data-retrieval","title":"Data Retrieval","text":"<p>Automatic ObjectId Serialization</p> <p>Both fetch methods automatically convert MongoDB's native <code>_id</code> (which is an <code>ObjectId</code> object) into a standard Python <code>str</code>. This prevents serialization errors when passing this data to JSON-based REST APIs or frontend components.</p> <ul> <li> <p><code>fetch_one(idx: str, filter={})</code></p> <ul> <li>Description: Retrieves a single document where the document's <code>\"id\"</code> field matches the provided <code>idx</code> string. </li> <li>Arguments: * <code>idx</code> (str): The specific ID to search for.<ul> <li><code>filter</code> (dict, optional): Additional query parameters to narrow the search.</li> </ul> </li> <li>Returns: A dictionary representing the document, or <code>None</code>.</li> </ul> </li> <li> <p><code>fetch_all(filter={}, limit=50)</code></p> <ul> <li>Description: Retrieves multiple documents matching the filter. Results are automatically sorted by <code>_id</code> in descending order (newest first).</li> <li>Arguments:<ul> <li><code>filter</code> (dict, optional): Query parameters. Defaults to an empty dict (fetch all).</li> <li><code>limit</code> (int, optional): Maximum number of documents to return. Defaults to 50.</li> </ul> </li> <li>Returns: A list of document dictionaries.</li> </ul> </li> </ul>"},{"location":"appendix/code_docs/connectors/mongo/#data-mutation","title":"Data Mutation","text":"<ul> <li><code>add_one(data: dict)</code><ul> <li>Inserts a single document dictionary into the collection.</li> </ul> </li> <li><code>add_many(data: list)</code><ul> <li>Inserts a list of document dictionaries into the collection in a single batch operation.</li> </ul> </li> <li><code>update_one(idx: str, data: dict, filter={})</code><ul> <li>Updates a single document where <code>\"id\"</code> matches <code>idx</code>. Uses the MongoDB <code>$set</code> operator to update only the fields provided in the <code>data</code> dictionary.</li> </ul> </li> <li><code>update_many(data: dict)</code><ul> <li>Updates multiple documents. (Note: Based on the current implementation, it specifically looks for documents matching <code>{\"id\": data[\"id\"]}</code> and applies the <code>$set</code> operator).</li> </ul> </li> </ul>"},{"location":"appendix/code_docs/connectors/mongo/#usage-example","title":"Usage Example","text":"<pre><code>from your_module import MongoConnect\n\n# 1. First instantiation (creates a new connection)\ndb_client = MongoConnect(\n    uri=\"mongodb://localhost:27017\",\n    db=\"ikegai_core\",\n    collection_name=\"agent_memory\"\n)\n\n# Insert data\ndb_client.add_one({\"id\": \"agent_001\", \"status\": \"idle\", \"task\": None})\n\n# 2. Second instantiation elsewhere in the code (reuses the alive connection)\nsame_client = MongoConnect(\n    uri=\"mongodb://localhost:27017\",\n    db=\"ikegai_core\",\n    collection_name=\"agent_memory\"\n)\n\n# Fetch data (ObjectIds are cleanly converted to strings)\nrecords = same_client.fetch_all(filter={\"status\": \"idle\"}, limit=10)\nprint(records)\n</code></pre>"}]}